{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dchec18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTT3CdbJJeYk"
      },
      "source": [
        "**Beam-ის დაყენება; საჭირო იმპორტები**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgBPknCq96_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99079f78-e96b-4ebb-b066-acdcffb1838b"
      },
      "source": [
        "%pip install --quiet apache-beam\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam as beam\n",
        "\n",
        "from apache_beam.dataframe import convert\n",
        "from apache_beam.dataframe.io import read_csv, to_csv\n",
        "\n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# თუ ეს error-ები და warning-ები გექნებათ, შეგიძლიათ დააიგნოროთ: \n",
        "# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.\n",
        "# ან\n",
        "# WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
        "# WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.8 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 842 kB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 57.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 249 kB 49.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 36.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 37.3 MB/s \n",
            "\u001b[?25h  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ekVqEHkK4HC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1c2c24-e256-4a53-e72d-5b4bbd0a47a2"
      },
      "source": [
        "pip show apache_beam"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: apache-beam\n",
            "Version: 2.34.0\n",
            "Summary: Apache Beam SDK for Python\n",
            "Home-page: https://beam.apache.org\n",
            "Author: Apache Software Foundation\n",
            "Author-email: dev@beam.apache.org\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: hdfs, pyarrow, pydot, grpcio, oauth2client, httplib2, future, typing-extensions, dill, pymongo, crcmod, protobuf, pytz, requests, orjson, fastavro, avro-python3, numpy, python-dateutil\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlhZ8gVQbwb"
      },
      "source": [
        "# **ამოცანები**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTXe-i-gQ8OX"
      },
      "source": [
        "**ამოცანა #1 - სტუდენტების საბოლოო ნიშნების დათვლა**\n",
        "*(1 ქულა)*\n",
        "\n",
        "ამ ამოცანის მიზანია, რამოდენიმე სტუდენტისთვის ერთ-ერთი საგნის ფინალური ქულა დავთვალით და გავიგოთ, ჩააბარა თუ არა სტუდენტმა ეს საგანი. კრიტერიუმები მარტივია და სულ ორი კომპონენტია:\n",
        "\n",
        " 1) შუალედური გამოცდა, რომლის ქულებიც თითოეული სტუდენტისთვის მოცემულია midterm_grades ცვლადში. ეს არის tuple-ების მასივი. tuple-ის პირველი ელემენტია სტუდენტის სახელი, მეორე - გამოცდაში მიღებული ქულა;\n",
        " \n",
        " 2) ფინალური პროექტი, რომლის ქულებიც თითოეული სტუდენტისთვის მოცემულია project_grades ცვლადში. ესეც არის tuple-ების მასივი. \n",
        "\n",
        "საგნის ჩააბარების კრიტერიუმი: თუ სტუდენტის საშუალო ქულა 50.0-ია, მაშინ ვთვლით, რომ სტუდენტმა საგანი ჩააბარა.\n",
        "\n",
        "ამოცანაში უნდა დაითვალოთ სტუდენტების საშუალო ქულა ორივე კომპონენტში და საბოლოო შედეგი წარმოადგინოთ pcollection-ის სახით. ამ კოლექციის თითოეული ელემენტი უნდა იყოს დაფორმატებული ასეთნაირად:\n",
        "\n",
        "*Student1 has passed the subject with avg grade of 100.0*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "ამოცანა უნდა შეასრულოთ Beam-ის გამოყენებით. ამისთვის გაწვდით მინიმალისტურ ამოხსნას (უფრო სტრუქტურას), რომელიც თქვენ უნდა გაარჩიოთ და დაასრულოთ. გაითვალისწინეთ:\n",
        " * ამოხსნის ძირითადი ჩონჩხი უკვე აწყობილია. თქვენ საწყისი pcollection-ები სწორად უნდა დააგენერიროთ და დაასრულოთ apply_transforms ფუნქციის იმპლემენტაცია. apply_transforms არის ის ადგილი, სადაც ყველა მოთხოვნილი ტრანფორმაცია ხდება pcollection-ებზე.\n",
        " * ფორმატირებისთვის უნდა დაიხმაროთ კლასი StudentGrade. რა თქმა უნდა, მისი იმპლემენტაციაც თქვენი მოსაფიქრებელია. 🐱‍💻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYzvslNULppU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "d489acfd-eac0-4351-c599-fc996a2477fd"
      },
      "source": [
        "midterm_grades = [('Student1', 100.0),('Student2', 50.0),('Student3', 75.0)]\n",
        "project_grades = [('Student1', 100.0),('Student2', 50.0),('Student4', 90.0),('Student5', 99.0)]\n",
        "\n",
        "PASSING_GRADE = 50.0\n",
        "\n",
        "class StudentGrade:\n",
        "    \n",
        "  \n",
        "    def __init__(self, student_name, midterm_grade, project_grade):\n",
        "        self.student_name = student_name\n",
        "        self.avg_grade = float(((midterm_grade or 0) + (project_grade or 0))) / 2.0\n",
        "\n",
        "    def __str__(self):\n",
        "        grade_string = self.student_name + \" has \"\n",
        "        grade_string += \"passed\" if self.avg_grade >= PASSING_GRADE else \"failed\"\n",
        "        grade_string += \" the subject with avg grade of \" + str(self.avg_grade)\n",
        "        return grade_string\n",
        "\n",
        "def apply_transforms(midterm_grades, project_grades):\n",
        "\n",
        "    def cogbk_result_to_studentgrade(cgbk_result):\n",
        "        (name, grade) = cgbk_result\n",
        "        if len(grade['midterm']) == 0: \n",
        "            grade['midterm'].append(0)\n",
        "        if len(grade['project']) == 0: \n",
        "            grade['project'].append(0)   \n",
        "        return StudentGrade(name, grade['midterm'][0], grade['project'][0])\n",
        " \n",
        "    return ({'midterm': midterm_grades, 'project': project_grades}\n",
        "            | beam.CoGroupByKey()\n",
        "            | beam.Map(cogbk_result_to_studentgrade))\n",
        "          \n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "    midterm_collection = (p | 'Midterm' >> beam.Create(midterm_grades))\n",
        "    project_collection = (p | 'Project' >> beam.Create(project_grades))\n",
        "\n",
        "    (\n",
        "      apply_transforms(midterm_collection, project_collection)\n",
        "        | beam.Map(print)\n",
        "    )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student1 has passed the subject with avg grade of 100.0\n",
            "Student2 has passed the subject with avg grade of 50.0\n",
            "Student4 has failed the subject with avg grade of 45.0\n",
            "Student5 has failed the subject with avg grade of 49.5\n",
            "Student3 has failed the subject with avg grade of 37.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJdY-Xc_ggLJ"
      },
      "source": [
        "**ამოცანა #2 - ისევ და ისევ Marvel and DC: the good and the bad**\n",
        "*(2 ქულა)*\n",
        "\n",
        "\n",
        "უნდა ვიპოვოთ Top 10 ყველაზე ძლევამოსილი პერსონაჟი Marvel-ისა და DC-ის სამყაროებიდან Beam-ის გამოყენებით.\n",
        "\n",
        "Dataset არის თქვენთვის უკვე ნაცნობი (ავად თუ კარგად 😁). სულ გვაქვს ორი csv ფაილი:\n",
        "* characters_info.csv - პერსონაჟების შესახებ მონაცემები (სახელი, კეთილია/ბოროტია, სქესი, ფიზიკური აღწერილობა, რომელი სამყაროს ნაწილია და ა. შ. ველები: Name, Alignment, Gender, EyeColor, Race, HairColor,\tPublisher,\tSkinColor,\tHeight, Weight;\n",
        "* character_stats.csv - სხვადასხვა პერსონაჟის სტატისტიკური მონაცემები ველებით Name, Alignment, Intelligence, Strength, Speed, Durability, Power, Combat, Total (ჯამური ქულა ყველა კომპონენტში);\n",
        "\n",
        "დუბლირებებზე შეგიძლიათ არ იდარდოთ. ასევე, რა პერსონაჟებიც არიან ერთ ფაილში, იგივე პერსონაჟებია მეორეშიც და პირიქით. ფაილებში პერსონაჟები არა მარტო მარველის ან dc-ის, არამედ სხვა სამყაროებიდანაცაა (Tolkien, Harry Potter...) .\n",
        "\n",
        "რომ დავაკონკრეტოთ ამოცანა:\n",
        "  * უნდა მოვძებნოთ Top 10 ყველაზე ძლევამოსილი პერსონაჟები მხოლოდ და მხოლოდ Marvel-ის ან DC-ის სამყაროებიდან. რაც უფრო მაღალია Total ველის მნიშვნელობა (stats ფაილი), მით უფრო ძლევამოსილია პერსონაჟი;\n",
        "  * გვაინტერესებს Top 10 ყველაზე ძლევამოსილი პერსონაჟი როგორც კეთილ გმირებში (alignment = 'good'), ასევე ბოროტებში (alignment = 'bad') **ცალ-ცალკე**. ანუ ტოპ 10 ყველაზე ძლიერი, კეთილი პერსონაჟი და იგივე ბოროტებისთვისაც. ყველა სხვა alignment არ გვაინტერესებს;\n",
        "  * საბოლოო შედეგში გვინდა, რომ გვქონდეს მხოლოდ შემდეგი ველები: name, score (იგივე, რაც Total ველი), gender, race, universe. ეს შედეგები თითოეული aligment-ისთვის უნდა შევინახოთ ცალ-ცალკე ფაილში: top_10_bad_heroes.csv და top_10_good_heroes.csv. ანუ სულ ეს ორი output ფაილი გვექნება. ათეული დალაგებული უნდა იყოს score-ის კლებადობის მიხედვით.\n",
        "\n",
        "\n",
        "**ტექნიკური დეტალები:**\n",
        " \n",
        " ამოხსნა მთლიანად თქვენს ფანტაზიაზეა, მაგრამ გარკვეულ რჩევებს მაინც მოგცემთ:\n",
        " * ამ ამოცანას უფრო იოლად ამოხსნით, თუ dataframe-ებს გამოიყენებთ;\n",
        " * იმისთვის, რომ dataframe-ები ვიზუალურად ნახოთ, დაგჭირდებათ Interactive Beam და მისი Runner, რომელთა შესაბამისი იმპორტებიც უკვე უზრუნველყოფილია. ***ib.collect(df)***-ით შეძლებთ dataframe ნახოთ საჭიროების შემთხვევაში. Pipeline-ის დასაწყისი დატოვეთ ისე, როგორც მოგაწოდეთ;\n",
        " * Beam-ის DataFrame-ები Pandas-ისას სინტაქსურად ძალიან ჰგავს, ოღონდ გარკვეული შეზღუდვებიც აქვს. მაგალითად, ინდექსების გარეშე Beam-ს პრობლემები აქვს ზოგ სიტუაციაში (თუნდაც merge-ისას) და ამის გამო გარკვეული exception-ები შეიძლება გქონდეთ. ამიტომ ინდექსები გაითვალისწინეთ;\n",
        " * მნიშვნელოვანი საკითხი: Beam-ის Pipeline-ში არც ერთი ოპერაციის label არ უნდა მეორდებოდეს. თუ რაღაც მიზეზებით ერთი და იგივე ტრანსფორმაცია ორჯერ გაქვთ ფაიფლაინში და სახელი საერთოდ არ მიუთითეთ, default-ად ერთსა და იმავე სახელს მიანიჭებს Beam და გექნებათ exception. ამიტომ გაითვალისწინეთ, რომ სხვადასხვა label უნდა მიუწეროთ ყველა ტრანსფორმაციას. Label-ები ორნაირად შეიძლება მიეთითოს, სიტუაციიდან გამომდინარე. მაგალითად: **pipeline | 'ლეიბლის უნიკალური სახელი' >> ტრანსფორმაცია**  ან **beam.Map(..., label='უნიკალური სახელი')**;\n",
        " * class Alignment(Enum): უბრალოდ enum-ია, სადაც alignment-ები გვაქვს გადანომრილი. აუცილებელი არაა გამოყენება, სურვილისამებრ;\n",
        " * Top 10 პერსონაჟის საპოვნელად, სავარაუდოდ, დაგჭირდებათ beam.transforms.combiners მოდულის ფუნქციები. თუ სხვა იდეა არ გექნებათ, მაგალითად, შეგიძლიათ ნახოთ აი, ეს ფუნქცია: [Top.Of](https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Top). [სხვა მაგალითებიც](http://beam.apache.org/documentation/transforms/python/aggregation/top/)\n",
        " * აუცილებელი არაა, მაგრამ ყოველი შემთხვევისთვის: თუ თქვენი solution-ის რაღაც მომენტში გექნებათ სიტუაცია, სადაც pipeline-ში გაქვთ pcollection სიის სახით და გინდათ, მიიღოთ სათითაოდ ყველა ელემენტი ამ სიაში, შეგიძლიათ, გამოიყენოთ ჩვენ მიერ მოწოდებული პაწაწინა კლასი BreakList;\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRE2sG5PKzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112346a1-3b05-4f5f-e772-f8512ff786a9"
      },
      "source": [
        "class Alignment(Enum):\n",
        "  bad = 0 \n",
        "  good = 1\n",
        "  neutral = 2\n",
        "  undefined = 3\n",
        "\n",
        "class BreakList(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    for e in element:\n",
        "      yield e\n",
        "\n",
        "\n",
        "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
        "    stats_df = pipeline | 'Read stats csv' >> beam.dataframe.io.read_csv('character_stats.csv')\n",
        "    info_df = pipeline | 'Read info csv' >> beam.dataframe.io.read_csv('characters_info.csv')\n",
        "\n",
        "    info_df = info_df.drop('Alignment', axis=1)\n",
        "    data = stats_df.merge(\n",
        "        info_df.set_index('Name'),\n",
        "        right_index=True,\n",
        "        left_on='Name',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    data = data[['Alignment','Total', 'Gender', 'Race', 'Publisher']]\n",
        "    data = data.rename(columns = {\n",
        "          \"Total\" : \"score\",\n",
        "          \"Gender\" : \"gender\",\n",
        "          \"Race\" : \"race\",\n",
        "          \"Publisher\" : \"universe\"\n",
        "        }\n",
        "    )\n",
        "    cond1 = data['universe'] == 'DC Comics'\n",
        "    cond2 = data['universe'] == 'Marvel Comics'\n",
        "\n",
        "    data = data[(cond1 | cond2)]\n",
        "    data_good = data[data['Alignment'] == 'good'].drop(\"Alignment\", axis=1)\n",
        "    data_bad = data[data['Alignment'] == 'bad'].drop(\"Alignment\", axis=1)\n",
        "    data_good = data_good.nlargest(10, 'score', keep='any')\n",
        "    data_bad = data_bad.nlargest(10, 'score', keep='any')\n",
        "    data_good.to_csv('top_10_good_heroes.csv')\n",
        "    data_bad.to_csv('top_10_bad_heroes.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/apache_beam/dataframe/io.py:572: FutureWarning: WriteToFiles is experimental.\n",
            "  sink=lambda _: _WriteToPandasFileSink(\n",
            "/usr/local/lib/python3.7/dist-packages/apache_beam/io/fileio.py:550: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  p.options.view_as(GoogleCloudOptions).temp_location or\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VKBwXzIS5MZ"
      },
      "source": [
        "**ამოცანა #3 - Breaking News**\n",
        "*(3 ქულა)*\n",
        "\n",
        "დატასეტში bbc_news.csv მოცემული გვაქვს 1500-მდე მოკლე სტატია BBC News-იდან. თითოეულ სტატიას აქვს უნიკალური ID და კატეგორიის label-ი (tech, business, politics, entertainment, sport).\n",
        "\n",
        "მიზანია, რომ თითოეულ სტატიას დავუგენერიროთ tag-ები შემდეგი მარტივი კრიტერიუმით:  უნდა მოვძებნოთ 3 ყველაზე ხშირი სიტყვა ამ სტატიაში. აუცილებელია, რომ თითოეული სიტყვა მინიმუმ 10-ჯერ მაინც გვხვდებოდეს კონკრეტულ სტატიაში.\n",
        "\n",
        "გაითვალისწინეთ, რომ:\n",
        " * Tag-ები არ უნდა იყოს ე.წ. stopword-ები ან სიმბოლოები. მათი ჩამონათვალი არის ფაილში stopwords.txt;\n",
        " * Tag-ები არ შეიძლება იყოს რიცხვები;\n",
        " * Tag-ები არ შეიძლება იყოს ერთ ან ორ სიმბოლოიანი სიტყვები;\n",
        " * სიტყვებად ჩავთვალოთ შემდეგი სიმბოლოებით დაყოფილი ტოკენები: \n",
        "\n",
        "          whitespace\n",
        "\n",
        "          ,\n",
        "\n",
        "          :\n",
        "\n",
        "          !\n",
        "\n",
        "          ?\n",
        "\n",
        " * ტრანსფორმაციები უნდა შეასრულოთ, რასაკვირველია, Beam-ის გამოყენებით და არა უბრალოდ პითონის ხარჯზე. 😏\n",
        "\n",
        "საბოლოო resultset უნდა ჩაწეროთ csv ფაილად და უნდა შედგებოდეს ველებისგან: ***ArticleID, Category, Tag, Frequency***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Af8bLwTDp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99a0b44-94a5-42f7-d9db-4f9de7e9753c"
      },
      "source": [
        "class ExplodeWords(beam.DoFn):\n",
        "  def process(self, row):\n",
        "    exploded = []\n",
        "    word_counts = row['counts']\n",
        "    for word_count in word_counts:\n",
        "      exploded.append({\"ArticleId\" : row[\"ArticleId\"],\n",
        "                       \"Category\" :  row[\"Category\"],\n",
        "                       \"Tag\": word_count[0],\n",
        "                       \"Frequency\": word_count[1]})\n",
        "    return exploded\n",
        "\n",
        "\n",
        "class FilterText(beam.DoFn):\n",
        "  def process(self, row, stopwords):\n",
        "    word_counts = row['Text']\n",
        "    filtered_counts = Counter()\n",
        "    for word, count in word_counts.items():\n",
        "      if count >= 10 and len(word) > 2 and not word.isnumeric() and word not in stopwords:\n",
        "        filtered_counts[word] = count\n",
        "    row['counts'] = filtered_counts.most_common(3) \n",
        "    yield row\n",
        "\n",
        "\n",
        "#replace text with split version\n",
        "class SplitText(beam.DoFn):\n",
        "    def process(self, row):\n",
        "      text = row['Text']\n",
        "      words = re.split('\\s|,|\\.|\\:|\\!|\\?', text)\n",
        "      word_counts = Counter(words)\n",
        "      row['Text'] = word_counts\n",
        "      yield row\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "    news_df = pipeline | 'Read news csv' >> beam.dataframe.io.read_csv('bbc_news.csv')\n",
        "    news_pcol = convert.to_pcollection(news_df) | beam.Map(lambda x: dict(x._asdict()))\n",
        "    stopwords = (pipeline | 'Read stopwords' >> beam.io.ReadFromText('stopwords.txt'))\n",
        "    ans_pcol = (news_pcol |\n",
        "                \"Split text\" >> beam.ParDo(SplitText()) |\n",
        "                \"Filter words\" >> beam.ParDo(FilterText(), beam.pvalue.AsList(stopwords)) |\n",
        "                \"Explode rows\" >> beam.ParDo(ExplodeWords())\n",
        "                )\n",
        "    ans_df = convert.to_dataframe(ans_pcol | \"To Rows\" >> beam.Map(\n",
        "        lambda x: beam.Row(\n",
        "            ArticleId = x['ArticleId'],\n",
        "            Category = x['Category'],\n",
        "            Tag = x['Tag'],\n",
        "            Frequency = x['Frequency'],\n",
        "        )\n",
        "    ))\n",
        "    ans_df.to_csv(\"resultset.csv\", index=False)  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/apache_beam/dataframe/io.py:572: FutureWarning: WriteToFiles is experimental.\n",
            "  sink=lambda _: _WriteToPandasFileSink(\n",
            "/usr/local/lib/python3.7/dist-packages/apache_beam/io/fileio.py:550: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  p.options.view_as(GoogleCloudOptions).temp_location or\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "745De2WwWe_D"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "შესრულებული დავალება მოგვაწოდეთ ნოუთბუქის სახით.\n",
        "წარმატებები!"
      ]
    }
  ]
}